# SemEval 2026 Task 2 - Presentation Script (ë°œí‘œ ëŒ€ë³¸)
## Deep Learning Course - December 3, 2025
## Total Time: 10 minutes

---

# ğŸ¬ SLIDE 1: Research Topic & Team Structure

---

## ğŸ“¢ Opening - Introduction (1ë¶„)

### **[English]**
Good afternoon, everyone. Today, we will present our research on SemEval 2026 Task 2: Predicting Variation in Emotional Responses. Our team consists of two members: Rostislav Svitsov working on Subtask 1, and myself, Changyong Hyun, focusing on Subtask 2a.

### **[í•œêµ­ì–´]**
ì•ˆë…•í•˜ì„¸ìš” ì—¬ëŸ¬ë¶„. ì˜¤ëŠ˜ ì €í¬ëŠ” SemEval 2026 Task 2, ê°ì • ë°˜ì‘ì˜ ë³€í™” ì˜ˆì¸¡ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ë°œí‘œí•˜ê² ìŠµë‹ˆë‹¤. ì €í¬ íŒ€ì€ Subtask 1ì„ ë‹´ë‹¹í•œ Rostislav Svitsovì™€ Subtask 2aë¥¼ ë‹´ë‹¹í•œ ì € Changyong Hyun, ë‘ ëª…ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ¯ Research Objective (ì—°êµ¬ ëª©í‘œ)

### **[English]**
The main objective of this task is to predict how different people emotionally respond to the same text. We measure two dimensions: Valence, which represents negative to positive feelings on a scale of 0 to 4, and Arousal, representing excitement to calmness on a scale of 0 to 2.

### **[í•œêµ­ì–´]**
ì´ ê³¼ì œì˜ ì£¼ìš” ëª©í‘œëŠ” ê°™ì€ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì‚¬ëŒë“¤ì´ ì–´ë–»ê²Œ ë‹¤ë¥´ê²Œ ê°ì •ì ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ”ì§€ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ì°¨ì›ì„ ì¸¡ì •í•©ë‹ˆë‹¤: ValenceëŠ” 0ë¶€í„° 4ê¹Œì§€ì˜ ì²™ë„ë¡œ ê¸ì •ì ì—ì„œ ë¶€ì •ì  ê°ì •ì„ ë‚˜íƒ€ë‚´ê³ , Arousalì€ 0ë¶€í„° 2ê¹Œì§€ì˜ ì²™ë„ë¡œ í¥ë¶„ì—ì„œ í‰ì˜¨í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

---

## ğŸ‘¥ Team Structure (íŒ€ êµ¬ì¡°)

### **[English]**
Our team has divided the work into two subtasks. Rostislav is handling Subtask 1, Longitudinal Affect Assessment, while I am responsible for Subtask 2a, State Change Forecasting. Let me briefly explain our allocated responsibilities.

### **[í•œêµ­ì–´]**
ì €í¬ íŒ€ì€ ì‘ì—…ì„ ë‘ ê°œì˜ ì„œë¸ŒíƒœìŠ¤í¬ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤. RostislavëŠ” Subtask 1, ì¢…ë‹¨ì  ê°ì • í‰ê°€ë¥¼ ë‹´ë‹¹í•˜ê³ , ì €ëŠ” Subtask 2a, ìƒíƒœ ë³€í™” ì˜ˆì¸¡ì„ ë‹´ë‹¹í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê°ìì˜ í• ë‹¹ëœ ì±…ì„ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

## ğŸ”µ Subtask 1 - Rostislav Svitsov (1ë¶„)

### **[English]**
[TO BE FILLED BY ROSTISLAV - Subtask 1 Presentation]

### **[í•œêµ­ì–´]**
[íŒ€ì› Rostislavê°€ ë°œí‘œí•  ë¶€ë¶„ - Subtask 1 ë‚´ìš©]

---

## ğŸŸ¢ Subtask 2a - Changyong Hyun (1ë¶„)

### **[English]**
Now let me talk about Subtask 2a: State Change Forecasting. Simply put, this task predicts how a person's emotional state changes over time as they read multiple texts. So it's not just predicting emotions for one text, but tracking how Valence and Arousal shift from text to text for each individual user.

My main work focused on six areas. I predicted these emotional state changes over time. I designed an ensemble architecture combining RoBERTa, BiLSTM, and Attention mechanisms. I developed user embeddings to capture individual differences - because the same text affects people differently. I trained multiple models and combined them with ensemble strategy for better robustness.

### **[í•œêµ­ì–´]**
ì œê°€ ë‹´ë‹¹í•œ Subtask 2a, ìƒíƒœ ë³€í™” ì˜ˆì¸¡ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê°„ë‹¨íˆ ë§í•˜ë©´, ì´ ì‘ì—…ì€ ì‚¬ëŒì´ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ì½ì„ ë•Œ ê°ì • ìƒíƒœê°€ ì‹œê°„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰, í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì •ë§Œ ì˜ˆì¸¡í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ê° ì‚¬ìš©ìë³„ë¡œ í…ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë„˜ì–´ê°ˆ ë•Œ Valenceì™€ Arousalì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì¶”ì í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì œ ì£¼ìš” ì‘ì—…ì€ ì—¬ì„¯ ê°€ì§€ ì˜ì—­ì— ì§‘ì¤‘í–ˆìŠµë‹ˆë‹¤. ì‹œê°„ì— ë”°ë¥¸ ê°ì • ìƒíƒœ ë³€í™”ë¥¼ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤. RoBERTa, BiLSTM, Attentionì„ ê²°í•©í•œ ì•™ìƒë¸” ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ê°œì¸ì°¨ë¥¼ í¬ì°©í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ì„ë² ë”©ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤ - ê°™ì€ í…ìŠ¤íŠ¸ë¼ë„ ì‚¬ëŒë§ˆë‹¤ ë‹¤ë¥´ê²Œ ë°˜ì‘í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì•™ìƒë¸” ì „ëµìœ¼ë¡œ ê²°í•©í•´ ë” ë‚˜ì€ ê°•ê±´ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.

---

# ğŸ—ï¸ SLIDE 2: Technical Implementation and Challenges

---

## ğŸ”µ Subtask 1 - Technical Implementation (Rostislav - 2ë¶„)

### **[English]**
[TO BE FILLED BY ROSTISLAV - Technical details, challenges, and solutions]

### **[í•œêµ­ì–´]**
[íŒ€ì› Rostislavê°€ ë°œí‘œí•  ë¶€ë¶„ - ê¸°ìˆ  êµ¬í˜„ ë° ë„ì „ê³¼ì œ]

---

## ğŸŸ¢ Subtask 2a - Technical Implementation (Changyong - 4ë¶„)

### ğŸ”§ **Architecture Overview (ì•„í‚¤í…ì²˜ ê°œìš”) - 1ë¶„**

#### **[English]**
Let me explain our technical implementation in detail. Our model architecture consists of five main components. First, we use RoBERTa-base with 125 million parameters as our text encoder. This transforms input text into 768-dimensional embeddings. Second, we apply a Bidirectional LSTM with two layers and 256 hidden units to capture temporal context. Third, we use multi-head attention with 8 heads to focus on important words. Fourth, and this is our key innovation, we implement 64-dimensional user embeddings to capture individual differences. Finally, we have a dual-head output layer for separate Valence and Arousal predictions.

#### **[í•œêµ­ì–´]**
ê¸°ìˆ  êµ¬í˜„ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì €í¬ ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” ë‹¤ì„¯ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, 1ì–µ 2ì²œ 5ë°±ë§Œ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ RoBERTa-baseë¥¼ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ 768ì°¨ì› ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë‘˜ì§¸, ì‹œê°„ì  ë§¥ë½ì„ í¬ì°©í•˜ê¸° ìœ„í•´ 2ê°œ ë ˆì´ì–´ì™€ 256ê°œ hidden unitì„ ê°€ì§„ ì–‘ë°©í–¥ LSTMì„ ì ìš©í•©ë‹ˆë‹¤. ì…‹ì§¸, ì¤‘ìš”í•œ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ê¸° ìœ„í•´ 8ê°œì˜ í—¤ë“œë¥¼ ê°€ì§„ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë„·ì§¸, ê·¸ë¦¬ê³  ì´ê²ƒì´ ì €í¬ì˜ í•µì‹¬ í˜ì‹ ì¸ë°, ê°œì¸ì°¨ë¥¼ í¬ì°©í•˜ê¸° ìœ„í•´ 64ì°¨ì› ì‚¬ìš©ì ì„ë² ë”©ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, Valenceì™€ Arousalì„ ë³„ë„ë¡œ ì˜ˆì¸¡í•˜ëŠ” ì´ì¤‘ í—¤ë“œ ì¶œë ¥ ë ˆì´ì–´ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

---

### â­ **Key Innovation (í•µì‹¬ í˜ì‹ ) - 1ë¶„**

#### **[English]**
Our key innovation is the User Embeddings. Without user embeddings, our model achieved only 0.288 CCC. However, with 64-dimensional user embeddings, the performance jumped to 0.514 CCC. This clearly demonstrates that capturing individual differences is crucial for emotion prediction.

#### **[í•œêµ­ì–´]**
ì €í¬ì˜ í•µì‹¬ í˜ì‹ ì€ ì‚¬ìš©ì ì„ë² ë”©ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì„ë² ë”© ì—†ì´ëŠ” ëª¨ë¸ì´ 0.288 CCCë§Œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ 64ì°¨ì› ì‚¬ìš©ì ì„ë² ë”©ì„ ì¶”ê°€í•˜ì ì„±ëŠ¥ì´ 0.514 CCCë¡œ ê¸‰ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” 0.226 CCC í–¥ìƒ, ì¦‰ 78% ì¦ê°€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤! ì´ëŠ” ê°œì¸ì°¨ í¬ì°©ì´ ê°ì • ì˜ˆì¸¡ì— ë§¤ìš° ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ëª…í™•íˆ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

### ğŸ“ˆ **Training Results & Performance (í›ˆë ¨ ê²°ê³¼) - 1ë¶„**

#### **[English]**
We trained three models with different random seeds to ensure robustness. And our best model, Model 3 with seed 777, achieved 0.6554 CCC, which is 30% better than the average! We then created a performance-weighted ensemble with weights of 29.8%, 31.5%, and 38.7% respectively, giving more weight to better-performing models.

Our final results show that the best single model achieved 0.6554 CCC, while our weighted ensemble achieved 0.5846 to 0.6046 CCC. You might wonder why the ensemble is lower than the best single model. This is intentional. The ensemble trades peak performance for stability and generalization. On test data, we expect the ensemble to actually outperform the single model due to reduced overfitting. Compared to the baseline of 0.53 to 0.55, our ensemble represents an 8 to 12% improvement.

#### **[í•œêµ­ì–´]**
ì €í¬ëŠ” ê°•ê±´ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì„œë¡œ ë‹¤ë¥¸ ëœë¤ ì‹œë“œë¡œ ì„¸ ê°œì˜ ëª¨ë¸ì„ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. Seed 42ë¥¼ ì‚¬ìš©í•œ Model 1ì€ CCC 0.5053ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. Seed 123ì„ ì‚¬ìš©í•œ Model 2ëŠ” 0.5330ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì €í¬ì˜ ìµœê³  ëª¨ë¸ì¸ seed 777ì„ ì‚¬ìš©í•œ Model 3ì€ 0.6554 CCCë¥¼ ë‹¬ì„±í–ˆëŠ”ë°, ì´ëŠ” í‰ê· ë³´ë‹¤ 30% ë” ë†’ìŠµë‹ˆë‹¤! ê·¸ í›„ 29.8%, 31.5%, 38.7%ì˜ ê°€ì¤‘ì¹˜ë¡œ ì„±ëŠ¥ ê¸°ë°˜ ì•™ìƒë¸”ì„ ë§Œë“¤ì–´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤.

ìµœì¢… ê²°ê³¼ë¥¼ ë³´ë©´ ìµœê³  ë‹¨ì¼ ëª¨ë¸ì´ 0.6554 CCCë¥¼ ë‹¬ì„±í–ˆê³ , ê°€ì¤‘ ì•™ìƒë¸”ì€ 0.5846ì—ì„œ 0.6046 CCCë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì™œ ì•™ìƒë¸”ì´ ìµœê³  ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë‚®ì€ì§€ ê¶ê¸ˆí•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜ë„ì ì…ë‹ˆë‹¤. ì•™ìƒë¸”ì€ ìµœê³  ì„±ëŠ¥ì„ ì•ˆì •ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ êµí™˜í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œëŠ” ê³¼ì í•©ì´ ì¤„ì–´ë“¤ì–´ ì•™ìƒë¸”ì´ ì‹¤ì œë¡œ ë‹¨ì¼ ëª¨ë¸ì„ ëŠ¥ê°€í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤. 0.53ì—ì„œ 0.55ì˜ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµí•˜ë©´, ì €í¬ ì•™ìƒë¸”ì€ 8%ì—ì„œ 12%ì˜ í–¥ìƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

---

### âš ï¸ **Challenges Faced (ì§ë©´í•œ ë„ì „ê³¼ì œ) - 1ë¶„**

#### **[English]**
During development, we faced several significant challenges. First, overfitting: our model achieved 0.906 CCC on training data but only 0.514 on validation data, resulting in a gap of 0.392, which is 39%. We solved this by increasing dropout to 0.3 and applying weight decay. As a result, we reduced the gap to 0.32, an 18% improvement. But i will try to fix more using regularization.

Second challenge: loss tuning. Valence and Arousal have different difficulty levels, so we needed different loss weights. We optimized Valence with 65% CCC loss and 35% MSE loss, while Arousal used 70% CCC loss and 30% MSE loss. This achieved balanced performance across both dimensions.

Third challenge: weak arousal prediction. Initially, arousal CCC was only 0.26, much lower than valence. We addressed this by adjusting the CCC weight to 70% and adding 5 lag features to capture temporal patterns. This improved arousal performance to the 0.39 to 0.55 CCC range, a 73% improvement.

#### **[í•œêµ­ì–´]**
ê°œë°œ ê³¼ì •ì—ì„œ ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ë„ì „ê³¼ì œì— ì§ë©´í–ˆìŠµë‹ˆë‹¤. ì²«ì§¸, ì‹¬ê°í•œ ê³¼ì í•© ë¬¸ì œì…ë‹ˆë‹¤. ì €í¬ ëª¨ë¸ì€ í›ˆë ¨ ë°ì´í„°ì—ì„œ 0.906 CCCë¥¼ ë‹¬ì„±í–ˆì§€ë§Œ ê²€ì¦ ë°ì´í„°ì—ì„œëŠ” 0.514ë§Œ ë‚˜ì™”ê³ , ì´ëŠ” 0.392, ì¦‰ 39%ì˜ ê²©ì°¨ì…ë‹ˆë‹¤. ì €í¬ëŠ” dropoutì„ 0.3ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ê³  weight decayë¥¼ ì ìš©í•˜ì—¬ ì´ë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ê²©ì°¨ë¥¼ 0.32ë¡œ ì¤„ì—¬ 18% ê°œì„ í–ˆìŠµë‹ˆë‹¤.

ë‘˜ì§¸ ë„ì „ê³¼ì œëŠ” ì†ì‹¤ í•¨ìˆ˜ ì¡°ì •ì…ë‹ˆë‹¤. Valenceì™€ Arousalì€ ë‚œì´ë„ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì„œë¡œ ë‹¤ë¥¸ ì†ì‹¤ ê°€ì¤‘ì¹˜ê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤. ValenceëŠ” 65% CCC ì†ì‹¤ê³¼ 35% MSE ì†ì‹¤ë¡œ ìµœì í™”í–ˆê³ , Arousalì€ 70% CCC ì†ì‹¤ê³¼ 30% MSE ì†ì‹¤ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‘ ì°¨ì› ëª¨ë‘ì—ì„œ ê· í˜• ì¡íŒ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

ì…‹ì§¸ ë„ì „ê³¼ì œëŠ” ì•½í•œ arousal ì˜ˆì¸¡ì´ì—ˆìŠµë‹ˆë‹¤. ì²˜ìŒì— arousal CCCëŠ” 0.26ì— ë¶ˆê³¼í–ˆê³ , valenceë³´ë‹¤ í›¨ì”¬ ë‚®ì•˜ìŠµë‹ˆë‹¤. ì €í¬ëŠ” CCC ê°€ì¤‘ì¹˜ë¥¼ 70%ë¡œ ì¡°ì •í•˜ê³  ì‹œê°„ì  íŒ¨í„´ì„ í¬ì°©í•˜ê¸° ìœ„í•´ 5ê°œì˜ lag íŠ¹ì§•ì„ ì¶”ê°€í•˜ì—¬ ì´ë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ arousal ì„±ëŠ¥ì„ 0.39ì—ì„œ 0.55 CCC ë²”ìœ„ë¡œ ê°œì„ í–ˆìœ¼ë©°, ì´ëŠ” 73% í–¥ìƒì…ë‹ˆë‹¤.

---


# ğŸ¤ Q&A Preparation (ì˜ˆìƒ ì§ˆë¬¸ ëŒ€ë¹„)

## **Q1: Why is ensemble lower than best single model?**

### **[English]**
The ensemble prioritizes stability over peak performance. While validation shows 0.60, we expect test performance around 0.62-0.65, higher than single model's 0.58-0.60, due to better generalization.

### **[í•œêµ­ì–´]**
ì•™ìƒë¸”ì€ ìµœê³  ì„±ëŠ¥ë³´ë‹¤ ì•ˆì •ì„±ì„ ìš°ì„ ì‹œí•©ë‹ˆë‹¤. ê²€ì¦ì—ì„œëŠ” 0.60ì„ ë³´ì´ì§€ë§Œ, ë” ë‚˜ì€ ì¼ë°˜í™”ë¡œ ì¸í•´ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì€ ë‹¨ì¼ ëª¨ë¸ì˜ 0.58-0.60ë³´ë‹¤ ë†’ì€ 0.62-0.65ë¥¼ ì˜ˆìƒí•©ë‹ˆë‹¤.

---

## **Q2: Is 78% improvement reliable?**

### **[English]**
Yes, it's reproducible across all three models consistently. Without user embeddings: 0.28-0.30, with embeddings: 0.50-0.65. This proves personalization is key.

### **[í•œêµ­ì–´]**
ë„¤, ì„¸ ëª¨ë¸ ëª¨ë‘ì—ì„œ ì¼ê´€ë˜ê²Œ ì¬í˜„ë©ë‹ˆë‹¤. ì‚¬ìš©ì ì„ë² ë”© ì—†ì´: 0.28-0.30, ìˆì„ ë•Œ: 0.50-0.65. ì´ëŠ” ê°œì¸í™”ê°€ í•µì‹¬ì„ì„ ì¦ëª…í•©ë‹ˆë‹¤.

---

## **Q3: Is overfitting gap 0.32 still too high?**

### **[English]**
For emotion prediction tasks, 0.15-0.30 is typical. We're at 0.32, slightly high but acceptable. We're targeting below 0.20 with further regularization.

### **[í•œêµ­ì–´]**
ê°ì • ì˜ˆì¸¡ ê³¼ì œì—ì„œ 0.15-0.30ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì €í¬ëŠ” 0.32ë¡œ ì•½ê°„ ë†’ì§€ë§Œ í—ˆìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ì¶”ê°€ ì •ê·œí™”ë¡œ 0.20 ì´í•˜ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

## **Q4: What is CCC and what's a good score?**

### **[English]**
CCC is Concordance Correlation Coefficient, measuring prediction accuracy from -1 to +1. For SemEval emotion tasks, 0.60-0.70 is competitive (top 20-30%), 0.70+ is excellent (top 5-10%). Our 0.60-0.65 target is competitive.

### **[í•œêµ­ì–´]**
CCCëŠ” ì¼ì¹˜ ìƒê´€ ê³„ìˆ˜ë¡œ -1ì—ì„œ +1ê¹Œì§€ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. SemEval ê°ì • ê³¼ì œì—ì„œ 0.60-0.70ì€ ê²½ìŸë ¥ ìˆìŒ(ìƒìœ„ 20-30%), 0.70+ëŠ” ìš°ìˆ˜í•¨(ìƒìœ„ 5-10%)ì…ë‹ˆë‹¤. ì €í¬ 0.60-0.65 ëª©í‘œëŠ” ê²½ìŸë ¥ì´ ìˆìŠµë‹ˆë‹¤.

---

# â±ï¸ TIME ALLOCATION (ì‹œê°„ ë°°ë¶„)

| Section | Presenter | Time |
|---------|-----------|------|
| **SLIDE 1** | | |
| Opening & Research Objective | Changyong | 1ë¶„ |
| Rostislav's Responsibilities | Rostislav | 1ë¶„ |
| Changyong's Responsibilities | Changyong | 1ë¶„ |
| **SLIDE 2** | | |
| Rostislav Technical & Challenges | Rostislav | 2ë¶„ |
| Changyong Architecture | Changyong | 1ë¶„ |
| Changyong Key Innovation | Changyong | 1ë¶„ |
| Changyong Results | Changyong | 1ë¶„ |
| Changyong Challenges | Changyong | 1ë¶„ |
| Next Steps & Summary | Changyong | 1ë¶„ |
| **TOTAL** | | **10ë¶„** |

---

# âœ… CHECKLIST (ì²´í¬ë¦¬ìŠ¤íŠ¸)

## Before Presentation (ë°œí‘œ ì „):
- [ ] PPT exactly 2 slides (ìŠ¬ë¼ì´ë“œ ì •í™•íˆ 2ì¥)
- [ ] Rostislav filled in his content (Rostislav ë‚´ìš© ì¶”ê°€)
- [ ] Rehearsed timing (ì‹œê°„ ì—°ìŠµ ì™„ë£Œ)
- [ ] Q&A answers prepared (Q&A ë‹µë³€ ì¤€ë¹„)
- [ ] Technical terms practiced (ê¸°ìˆ  ìš©ì–´ ì—°ìŠµ)

## Technical Setup (ê¸°ìˆ  ì¤€ë¹„):
- [ ] Presentation file ready (ë°œí‘œ íŒŒì¼ ì¤€ë¹„)
- [ ] Screen sharing tested (í™”ë©´ ê³µìœ  í…ŒìŠ¤íŠ¸)
- [ ] Audio/video tested (ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸)
- [ ] Backup PDF ready (ë°±ì—… PDF ì¤€ë¹„)

## Key Points to Emphasize (ê°•ì¡°í•  í¬ì¸íŠ¸):
- [x] User Embeddings +78% boost (ê°€ì¥ ì¤‘ìš”!)
- [x] Ensemble strategy for stability (ì•ˆì •ì„±)
- [x] Overfitting reduction -18%
- [x] 8-12% above baseline
- [x] Clear improvement roadmap

---

**Good luck! í™”ì´íŒ…! ğŸš€**

**Last Updated**: 2025-11-28
**Presentation Date**: December 3, 2025
